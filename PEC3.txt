# =============================================================================
# PRÁCTICA 3 - DEEP LEARNING: Clasificación de Granos de Polen
# Parte 1: Exploración y Análisis Exploratorio del Dataset
# =============================================================================

# 1. Instalación de paquetes necesarios (incluyendo keras3)
pacotes <- c("keras3", "tensorflow", "ggplot2", "gridExtra", "dplyr")
new_packages <- pacotes[!(pacotes %in% installed.packages()[,"Package"])]
if(length(new_packages)) install.packages(new_packages, dependencies = TRUE)

# 2. Cargar librerías
library(keras3)
library(tensorflow)
library(ggplot2)
library(gridExtra)
library(dplyr, warn.conflicts = FALSE)

# 3. Configuración inicial (Solo si no lo has hecho antes)
#install_tensorflow()

# Configurar semilla para reproducibilidad
set.seed(123)
tensorflow::tf$random$set_seed(123)

# =============================================================================
# 1. CARGA Y ORGANIZACIÓN DEL DATASET
# =============================================================================

# Definir rutas al dataset
# AJUSTAR ESTAS RUTAS SEGÚN TU ESTRUCTURA DE CARPETAS
# data_dir <- "path/to/your/pollen_dataset"  # Carpeta principal
data_dir <- "/Users/darolmar/Desktop/anuka1200"  # Carpeta principal
class_a_dir <- file.path(data_dir, "TipoA_Kunzea")  # Primera especie
class_b_dir <- file.path(data_dir, "TipoB_Lepto")  # Segunda especie

# Listar archivos de imágenes
images_class_a <- list.files(class_a_dir, pattern = "\\.(jpg|jpeg|png)$", 
                             full.names = TRUE, ignore.case = TRUE)
images_class_b <- list.files(class_b_dir, pattern = "\\.(jpg|jpeg|png)$", 
                             full.names = TRUE, ignore.case = TRUE)

# Información básica del dataset
cat("=== INFORMACIÓN DEL DATASET ===\n")
cat("Imágenes Clase A:", length(images_class_a), "\n")
cat("Imágenes Clase B:", length(images_class_b), "\n")
cat("Total de imágenes:", length(images_class_a) + length(images_class_b), "\n")
cat("Balance de clases:", 
    round(length(images_class_a) / (length(images_class_a) + length(images_class_b)), 3), "\n\n")

# =============================================================================
# 2. ANÁLISIS DE DIMENSIONES Y CARACTERÍSTICAS
# =============================================================================

# Función para cargar una imagen y obtener sus dimensiones
get_image_info <- function(img_path) {
  img <- image_load(img_path, color_mode = "grayscale")
  img_array <- image_to_array(img)
  return(dim(img_array))
}

# Muestrear algunas imágenes para verificar dimensiones
sample_images <- c(head(images_class_a, 5), head(images_class_b, 5))
dimensions_list <- lapply(sample_images, get_image_info)

cat("=== DIMENSIONES DE LAS IMÁGENES ===\n")
print(dimensions_list[1:3])

# Verificar si todas tienen las mismas dimensiones
unique_dims <- unique(dimensions_list)
cat("\nNúmero de dimensiones únicas:", length(unique_dims), "\n")
if(length(unique_dims) == 1) {
  cat("✓ Todas las imágenes tienen dimensiones consistentes:", 
      paste(unique_dims[[1]], collapse = " x "), "\n\n")
} else {
  cat("⚠ ADVERTENCIA: Las imágenes tienen dimensiones variables\n")
  cat("Se requerirá redimensionamiento\n\n")
}

# =============================================================================
# 3. VISUALIZACIÓN DE MUESTRAS REPRESENTATIVAS
# =============================================================================

# Función para visualizar imágenes
plot_sample_images <- function(image_paths, class_name, n_samples = 6) {
  par(mfrow = c(2, 3), mar = c(1, 1, 2, 1))
  
  sample_indices <- sample(1:length(image_paths), min(n_samples, length(image_paths)))
  
  for(i in sample_indices) {
    img <- image_load(image_paths[i], color_mode = "grayscale", target_size = c(150, 150))
    img_array <- image_to_array(img)
    img_array <- img_array / 255  # Normalizar para visualización
    
    # Convertir a formato para plot
    img_matrix <- img_array[,,1]
    
    image(t(img_matrix[nrow(img_matrix):1,]), 
          col = gray.colors(256),
          axes = FALSE,
          main = paste(class_name, "-", i))
  }
  par(mfrow = c(1, 1))
}

cat("=== VISUALIZACIÓN DE MUESTRAS ===\n")
cat("Generando visualizaciones...\n\n")

# Visualizar Clase A
plot_sample_images(images_class_a, "Clase A", 6)
# Visualizar Clase B  
plot_sample_images(images_class_b, "Clase B", 6)

# =============================================================================
# 4. ANÁLISIS ESTADÍSTICO DE INTENSIDAD DE PÍXELES
# =============================================================================

# Función para calcular estadísticas de una imagen
get_pixel_stats <- function(img_path) {
  img <- image_load(img_path, color_mode = "grayscale", target_size = c(150, 150))
  img_array <- image_to_array(img) / 255
  
  return(data.frame(
    mean_intensity = mean(img_array),
    sd_intensity = sd(img_array),
    min_intensity = min(img_array),
    max_intensity = max(img_array)
  ))
}

# Calcular estadísticas para una muestra de cada clase
n_sample <- min(50, length(images_class_a), length(images_class_b))

cat("=== ANÁLISIS DE INTENSIDAD DE PÍXELES ===\n")
cat("Calculando estadísticas para", n_sample, "imágenes por clase...\n")

stats_a <- do.call(rbind, lapply(head(images_class_a, n_sample), get_pixel_stats))
stats_a$class <- "A"

stats_b <- do.call(rbind, lapply(head(images_class_b, n_sample), get_pixel_stats))
stats_b$class <- "B"

stats_combined <- rbind(stats_a, stats_b)

# Resumen estadístico por clase
cat("\nClase A - Estadísticas de intensidad:\n")
print(summary(stats_a[, 1:4]))

cat("\nClase B - Estadísticas de intensidad:\n")
print(summary(stats_b[, 1:4]))

# Visualización comparativa
p1 <- ggplot(stats_combined, aes(x = class, y = mean_intensity, fill = class)) +
  geom_boxplot() +
  labs(title = "Distribución de Intensidad Media por Clase",
       x = "Clase", y = "Intensidad Media") +
  theme_minimal() +
  scale_fill_manual(values = c("A" = "steelblue", "B" = "coral"))

p2 <- ggplot(stats_combined, aes(x = class, y = sd_intensity, fill = class)) +
  geom_boxplot() +
  labs(title = "Distribución de Desviación Estándar por Clase",
       x = "Clase", y = "Desviación Estándar") +
  theme_minimal() +
  scale_fill_manual(values = c("A" = "steelblue", "B" = "coral"))

grid.arrange(p1, p2, ncol = 2)

# =============================================================================
# 5. PREPARACIÓN DE METADATOS PARA MODELADO
# =============================================================================

# Crear dataframe con todas las rutas y etiquetas
all_images <- c(images_class_a, images_class_b)
all_labels <- c(rep(0, length(images_class_a)), 
                rep(1, length(images_class_b)))

metadata <- data.frame(
  image_path = all_images,
  label = all_labels,
  class_name = ifelse(all_labels == 0, "Clase_A", "Clase_B"),
  stringsAsFactors = FALSE
)

# Mezclar aleatoriamente
set.seed(123)
metadata <- metadata[sample(nrow(metadata)), ]

cat("\n=== METADATA PREPARADO ===\n")
cat("Total de imágenes procesadas:", nrow(metadata), "\n")
cat("Distribución de etiquetas:\n")
print(table(metadata$class_name))

# =============================================================================
# 6. DIVISIÓN EN CONJUNTOS DE ENTRENAMIENTO, VALIDACIÓN Y TEST
# =============================================================================

# Proporción: 70% train, 15% validation, 15% test
n_total <- nrow(metadata)
n_train <- floor(0.70 * n_total)
n_val <- floor(0.15 * n_total)
n_test <- n_total - n_train - n_val

train_data <- metadata[1:n_train, ]
val_data <- metadata[(n_train + 1):(n_train + n_val), ]
test_data <- metadata[(n_train + n_val + 1):n_total, ]

cat("\n=== DIVISIÓN DE DATOS ===\n")
cat("Entrenamiento:", nrow(train_data), "imágenes\n")
cat("Validación:", nrow(val_data), "imágenes\n")
cat("Test:", nrow(test_data), "imágenes\n\n")

# Verificar balance en cada conjunto
cat("Balance en conjunto de entrenamiento:\n")
print(prop.table(table(train_data$class_name)))
cat("\nBalance en conjunto de validación:\n")
print(prop.table(table(val_data$class_name)))
cat("\nBalance en conjunto de test:\n")
print(prop.table(table(test_data$class_name)))

# =============================================================================
# 7. GUARDAR RESULTADOS DE LA EXPLORACIÓN
# =============================================================================

# Guardar metadata para uso posterior
saveRDS(list(
  train = train_data,
  val = val_data,
  test = test_data,
  img_dims = unique_dims[[1]]
), "dataset_metadata.rds")

cat("\n✓ Exploración completada. Metadata guardado en 'dataset_metadata.rds'\n")
cat("\n=== CONCLUSIONES DE LA EXPLORACIÓN ===\n")
cat("1. Dataset con", n_total, "imágenes en escala de grises\n")
cat("2. Balance de clases:", 
    ifelse(abs(length(images_class_a) - length(images_class_b)) < 10, 
           "equilibrado", "ligeramente desbalanceado"), "\n")
cat("3. Imágenes con dimensiones:", paste(unique_dims[[1]][1:2], collapse = "x"), "\n")
cat("4. Preparado para iniciar modelado\n")


# =============================================================================
# Parte 2: Modelado Progresivo (NET-1 → CNN Avanzadas) - ACTUALIZADO KERAS 3
# =============================================================================

# =============================================================================
# 1. CONFIGURACIÓN GLOBAL Y CARGA DE METADATA
# =============================================================================

metadata <- readRDS("dataset_metadata.rds")
train_data <- metadata$train
val_data <- metadata$val
test_data <- metadata$test

IMG_HEIGHT <- metadata$img_dims[1]
IMG_WIDTH <- metadata$img_dims[2]
BATCH_SIZE <- 32
EPOCHS <- 50

# =============================================================================
# 2. GENERADORES DE DATOS (VERSION KERAS 3)
# =============================================================================

# En Keras 3, la normalización y el aumento se definen como capas o funciones del dataset
normalization_layer <- layer_rescaling(scale = 1/255)

data_augmentation <- keras_model_sequential() %>%
  layer_random_flip("horizontal_and_vertical") %>%
  layer_random_rotation(factor = 0.11) %>% 
  layer_random_translation(height_factor = 0.2, width_factor = 0.2) %>%
  layer_random_zoom(factor = 0.2)

# Crear Datasets (Sustituye a flow_images_from_directory)
# Nota: Keras 3 usa el directorio raíz y detecta subcarpetas como clases automáticamente
full_train_ds <- image_dataset_from_directory(
  data_dir,
  label_mode = "binary",
  color_mode = "grayscale",
  image_size = c(IMG_HEIGHT, IMG_WIDTH),
  batch_size = BATCH_SIZE,
  seed = 123,
  validation_split = 0.2, # Usamos el split nativo para asegurar compatibilidad
  subset = "training"
)

full_val_ds <- image_dataset_from_directory(
  data_dir,
  label_mode = "binary",
  color_mode = "grayscale",
  image_size = c(IMG_HEIGHT, IMG_WIDTH),
  batch_size = BATCH_SIZE,
  seed = 123,
  validation_split = 0.2,
  subset = "validation"
)

# Aplicar transformaciones (Pipeline optimizado)
train_dataset_no_aug <- full_train_ds %>% 
  dataset_map(function(x, y) list(normalization_layer(x), y)) %>%
  dataset_prefetch(buffer_size = tf$data$AUTOTUNE)

train_dataset_aug <- full_train_ds %>% 
  dataset_map(function(x, y) list(data_augmentation(normalization_layer(x), training = TRUE), y)) %>%
  dataset_prefetch(buffer_size = tf$data$AUTOTUNE)

validation_dataset <- full_val_ds %>% 
  dataset_map(function(x, y) list(normalization_layer(x), y)) %>%
  dataset_prefetch(buffer_size = tf$data$AUTOTUNE)

# =============================================================================
# 3. VISUALIZAR AUGMENTATION (ACTUALIZADO)
# =============================================================================

visualize_augmentation <- function(dataset, n_samples = 9) {
  batch <- dataset %>% dataset_take(1) %>% as_iterator() %>% iter_next()
  images <- batch[[1]]
  
  par(mfrow = c(3, 3), mar = c(1, 1, 2, 1))
  for(i in 1:min(n_samples, dim(images)[1])) {
    img_display <- as.array(images[i,,,1])
    image(t(img_display[nrow(img_display):1,]), 
          col = gray.colors(256), axes = FALSE,
          main = paste("Augmented", i))
  }
  par(mfrow = c(1, 1))
}

visualize_augmentation(train_dataset_aug)

# =============================================================================
# 4. MODELO 1: NET-1 (REGRESIÓN LOGÍSTICA)
# =============================================================================

model_1 <- keras_model_sequential() %>%
  layer_flatten(input_shape = c(IMG_HEIGHT, IMG_WIDTH, 1)) %>%
  layer_dense(units = 1, activation = "sigmoid")

model_1 %>% compile(optimizer = optimizer_adam(), loss = "binary_crossentropy", metrics = "accuracy")

history_1 <- model_1 %>% fit(train_dataset_no_aug, epochs = EPOCHS, validation_data = validation_dataset, verbose = 2)

save_model(model_1, "model_1_net1.keras")

# =============================================================================
# 5. MODELO 2: RED CON CAPAS OCULTAS
# =============================================================================

model_2 <- keras_model_sequential() %>%
  layer_flatten(input_shape = c(IMG_HEIGHT, IMG_WIDTH, 1)) %>%
  layer_dense(units = 256, activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 1, activation = "sigmoid")

model_2 %>% compile(optimizer = optimizer_adam(), loss = "binary_crossentropy", metrics = "accuracy")

history_2 <- model_2 %>% fit(train_dataset_no_aug, epochs = EPOCHS, validation_data = validation_dataset, verbose = 2)

# =============================================================================
# 6. MODELO 3: CNN BASE (CON AUGMENTATION)
# =============================================================================

model_3 <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu", input_shape = c(IMG_HEIGHT, IMG_WIDTH, 1)) %>%
  layer_max_pooling_2d() %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d() %>%
  layer_flatten() %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model_3 %>% compile(optimizer = optimizer_adam(), loss = "binary_crossentropy", metrics = "accuracy")

history_3 <- model_3 %>% fit(train_dataset_aug, epochs = EPOCHS, validation_data = validation_dataset, verbose = 2)

# =============================================================================
# 7. MODELO 4: CNN CON BATCH NORMALIZATION
# =============================================================================

model_4 <- keras_model_sequential() %>%
  layer_conv_2d(32, c(3,3), activation = "relu", input_shape = c(IMG_HEIGHT, IMG_WIDTH, 1)) %>%
  layer_batch_normalization() %>%
  layer_max_pooling_2d() %>%
  layer_flatten() %>%
  layer_dense(1, activation = "sigmoid")

model_4 %>% compile(optimizer = optimizer_adam(), loss = "binary_crossentropy", metrics = "accuracy")

history_4 <- model_4 %>% fit(train_dataset_aug, epochs = EPOCHS, validation_data = validation_dataset, verbose = 2)

# =============================================================================
# 8. MODELO 5: CNN CON GAP
# =============================================================================

model_5 <- keras_model_sequential() %>%
  layer_conv_2d(32, c(3,3), activation = "relu", input_shape = c(IMG_HEIGHT, IMG_WIDTH, 1)) %>%
  layer_global_average_pooling_2d() %>%
  layer_dense(1, activation = "sigmoid")

model_5 %>% compile(optimizer = optimizer_adam(), loss = "binary_crossentropy", metrics = "accuracy")

history_5 <- model_5 %>% fit(train_dataset_aug, epochs = EPOCHS, validation_data = validation_dataset, verbose = 2)

# =============================================================================
# 9. COMPARACIÓN FINAL
# =============================================================================

results <- data.frame(
  Modelo = c("NET-1", "MLP", "CNN Base", "CNN + BN", "CNN + GAP"),
  Val_Accuracy = c(
    tail(history_1$metrics$val_accuracy, 1),
    tail(history_2$metrics$val_accuracy, 1),
    tail(history_3$metrics$val_accuracy, 1),
    tail(history_4$metrics$val_accuracy, 1),
    tail(history_5$metrics$val_accuracy, 1)
  )
)
print(results)
